// ---------------------------------------------------------------------------------------
//                                     ILGPU-AOT
//                        Copyright (c) 2024-2025 ILGPU-AOT Project
//
// Developed by:           Michael Ivertowski
//
// File: IntelNPUBenchmarks.cs
//
// This file is part of ILGPU-AOT and is distributed under the University of Illinois Open
// Source License. See LICENSE.txt for details.
// ---------------------------------------------------------------------------------------

using BenchmarkDotNet.Attributes;
using ILGPU;
using ILGPU.Runtime;
using System;

#if ENABLE_INTEL_NPU
using ILGPU.Intel.NPU;
#endif

namespace ILGPU.Benchmarks.Benchmarks
{
    /// <summary>
    /// Benchmarks for Intel Neural Processing Unit (NPU) Phase 7 AI acceleration capabilities.
    /// </summary>
    [MemoryDiagnoser]
    [SimpleJob]
    public class IntelNPUBenchmarks : IDisposable
    {
        private Context? context;
        private Accelerator? cpuAccelerator;
        private Accelerator? gpuAccelerator;
#if ENABLE_INTEL_NPU
        private NPUAccelerator? npuAccelerator;
#endif
        private float[]? inputData;
        private float[]? weightsData;
        private float[]? biasData;

        [Params(256, 512, 1024, 2048)]
        public int NetworkSize { get; set; }

        [Params(1, 8, 16, 32)]
        public int BatchSize { get; set; }

        [GlobalSetup]
        public void Setup()
        {
            context = Context.CreateDefault();
            cpuAccelerator = context.GetPreferredDevice(preferCPU: true).CreateAccelerator(context);

            // Try to get GPU accelerator
            foreach (var device in context.Devices)
            {
                if (device.AcceleratorType != AcceleratorType.CPU)
                {
                    gpuAccelerator = device.CreateAccelerator(context);
                    break;
                }
            }

#if ENABLE_INTEL_NPU
            // Try to get Intel NPU
            try
            {
                if (NPUCapabilities.DetectNPU() != NPUGeneration.None)
                {
                    npuAccelerator = new NPUAccelerator(context);
                }
            }
            catch
            {
                npuAccelerator = null;
            }
#endif

            // Initialize test data
            var totalElements = NetworkSize * NetworkSize;
            inputData = new float[totalElements];
            weightsData = new float[totalElements];
            biasData = new float[NetworkSize];

            var random = new Random(42);
            for (int i = 0; i < totalElements; i++)
            {
                inputData[i] = (float)(random.NextDouble() * 2.0 - 1.0); // Range [-1, 1]
                weightsData[i] = (float)(random.NextGaussian(0, 0.1)); // Xavier initialization
            }

            for (int i = 0; i < NetworkSize; i++)
            {
                biasData[i] = (float)(random.NextDouble() * 0.1);
            }
        }

        [GlobalCleanup]
        public void Cleanup()
        {
#if ENABLE_INTEL_NPU
            npuAccelerator?.Dispose();
#endif
            gpuAccelerator?.Dispose();
            cpuAccelerator?.Dispose();
            context?.Dispose();
        }

        [Benchmark(Baseline = true)]
        public float CPU_NeuralNetwork()
        {
            // Simple fully connected layer on CPU
            var kernel = cpuAccelerator!.LoadAutoGroupedStreamKernel<Index2D, ArrayView2D<float, Stride2D.DenseX>, ArrayView2D<float, Stride2D.DenseX>, ArrayView<float>, ArrayView2D<float, Stride2D.DenseX>, int>(
                (index, input, weights, bias, output, inputSize) =>
                {
                    var sum = 0.0f;
                    for (int k = 0; k < inputSize; k++)
                    {
                        sum += input[index.X, k] * weights[index.Y, k];
                    }
                    output[index.X, index.Y] = Math.Max(0.0f, sum + bias[index.Y]); // ReLU activation
                });

            var batchInputSize = Math.Min(BatchSize, NetworkSize);
            var inputSize = Math.Min(NetworkSize, NetworkSize);
            var outputSize = NetworkSize / 2;

            using var inputBuffer = cpuAccelerator.Allocate2DDenseX<float>(new Index2D(batchInputSize, inputSize));
            using var weightBuffer = cpuAccelerator.Allocate2DDenseX<float>(new Index2D(outputSize, inputSize));
            using var biasBuffer = cpuAccelerator.Allocate1D<float>(outputSize);
            using var outputBuffer = cpuAccelerator.Allocate2DDenseX<float>(new Index2D(batchInputSize, outputSize));

            inputBuffer.CopyFromCPU(inputData!.AsSpan(0, batchInputSize * inputSize));
            weightBuffer.CopyFromCPU(weightsData!.AsSpan(0, outputSize * inputSize));
            biasBuffer.CopyFromCPU(biasData!.AsSpan(0, outputSize));

            kernel(new Index2D(batchInputSize, outputSize), inputBuffer.View, weightBuffer.View, biasBuffer.View, outputBuffer.View, inputSize);
            cpuAccelerator.Synchronize();

            var result = outputBuffer.GetAsArray2D();
            return result[0, 0];
        }

        [Benchmark]
        public float GPU_NeuralNetwork()
        {
            if (gpuAccelerator == null)
                return CPU_NeuralNetwork();

            var kernel = gpuAccelerator.LoadAutoGroupedStreamKernel<Index2D, ArrayView2D<float, Stride2D.DenseX>, ArrayView2D<float, Stride2D.DenseX>, ArrayView<float>, ArrayView2D<float, Stride2D.DenseX>, int>(
                (index, input, weights, bias, output, inputSize) =>
                {
                    var sum = 0.0f;
                    for (int k = 0; k < inputSize; k++)
                    {
                        sum += input[index.X, k] * weights[index.Y, k];
                    }
                    output[index.X, index.Y] = Math.Max(0.0f, sum + bias[index.Y]);
                });

            var batchInputSize = Math.Min(BatchSize, NetworkSize);
            var inputSize = Math.Min(NetworkSize, NetworkSize);
            var outputSize = NetworkSize / 2;

            using var inputBuffer = gpuAccelerator.Allocate2DDenseX<float>(new Index2D(batchInputSize, inputSize));
            using var weightBuffer = gpuAccelerator.Allocate2DDenseX<float>(new Index2D(outputSize, inputSize));
            using var biasBuffer = gpuAccelerator.Allocate1D<float>(outputSize);
            using var outputBuffer = gpuAccelerator.Allocate2DDenseX<float>(new Index2D(batchInputSize, outputSize));

            inputBuffer.CopyFromCPU(inputData!.AsSpan(0, batchInputSize * inputSize));
            weightBuffer.CopyFromCPU(weightsData!.AsSpan(0, outputSize * inputSize));
            biasBuffer.CopyFromCPU(biasData!.AsSpan(0, outputSize));

            kernel(new Index2D(batchInputSize, outputSize), inputBuffer.View, weightBuffer.View, biasBuffer.View, outputBuffer.View, inputSize);
            gpuAccelerator.Synchronize();

            var result = outputBuffer.GetAsArray2D();
            return result[0, 0];
        }

#if ENABLE_INTEL_NPU
        [Benchmark]
        public float NPU_InferenceOptimized()
        {
            if (npuAccelerator == null)
                return CPU_NeuralNetwork();

            try
            {
                var inputSize = Math.Min(NetworkSize, 1024);
                var outputSize = inputSize / 2;

                // Create NPU-optimized neural network operation
                var operation = new NPUInferenceOperation(
                    inputSize: inputSize,
                    outputSize: outputSize,
                    batchSize: BatchSize,
                    precision: NPUPrecision.FP16 // NPU optimized for FP16
                );

                using var inputTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, inputSize });
                using var weightTensor = npuAccelerator.CreateTensor<float>(new[] { outputSize, inputSize });
                using var biasTensor = npuAccelerator.CreateTensor<float>(new[] { outputSize });
                using var outputTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, outputSize });

                inputTensor.CopyFromHost(inputData!.AsSpan(0, Math.Min(BatchSize * inputSize, inputData.Length)));
                weightTensor.CopyFromHost(weightsData!.AsSpan(0, Math.Min(outputSize * inputSize, weightsData.Length)));
                biasTensor.CopyFromHost(biasData!.AsSpan(0, Math.Min(outputSize, biasData.Length)));

                npuAccelerator.Execute(operation, 
                    new[] { inputTensor, weightTensor, biasTensor }, 
                    new[] { outputTensor });
                npuAccelerator.Synchronize();

                var result = outputTensor.CopyToHost();
                return result[0];
            }
            catch
            {
                return CPU_NeuralNetwork();
            }
        }

        [Benchmark]
        public float NPU_ConvolutionalLayer()
        {
            if (npuAccelerator == null)
                return CPU_NeuralNetwork();

            try
            {
                var imageSize = 64; // Small image for benchmark
                var inputChannels = 3;
                var outputChannels = 16;
                var kernelSize = 3;

                var operation = new NPUConvolution2DOperation(
                    inputChannels: inputChannels,
                    outputChannels: outputChannels,
                    kernelSize: kernelSize,
                    stride: 1,
                    padding: 1,
                    precision: NPUPrecision.INT8 // NPU efficient for INT8 convolutions
                );

                using var inputTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, inputChannels, imageSize, imageSize });
                using var kernelTensor = npuAccelerator.CreateTensor<float>(new[] { outputChannels, inputChannels, kernelSize, kernelSize });
                using var outputTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, outputChannels, imageSize, imageSize });

                var inputElements = BatchSize * inputChannels * imageSize * imageSize;
                var kernelElements = outputChannels * inputChannels * kernelSize * kernelSize;

                inputTensor.CopyFromHost(inputData!.AsSpan(0, Math.Min(inputElements, inputData.Length)));
                kernelTensor.CopyFromHost(weightsData!.AsSpan(0, Math.Min(kernelElements, weightsData.Length)));

                npuAccelerator.Execute(operation, new[] { inputTensor, kernelTensor }, new[] { outputTensor });
                npuAccelerator.Synchronize();

                var result = outputTensor.CopyToHost();
                return result[0];
            }
            catch
            {
                return CPU_NeuralNetwork();
            }
        }

        [Benchmark]
        public float NPU_TransformerAttention()
        {
            if (npuAccelerator == null)
                return CPU_NeuralNetwork();

            try
            {
                var sequenceLength = Math.Min(128, NetworkSize / 8);
                var hiddenSize = Math.Min(256, NetworkSize / 4);
                var numHeads = 8;

                var operation = new NPUMultiHeadAttentionOperation(
                    hiddenSize: hiddenSize,
                    numHeads: numHeads,
                    sequenceLength: sequenceLength,
                    precision: NPUPrecision.BF16 // BF16 for transformer models
                );

                using var queryTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, sequenceLength, hiddenSize });
                using var keyTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, sequenceLength, hiddenSize });
                using var valueTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, sequenceLength, hiddenSize });
                using var outputTensor = npuAccelerator.CreateTensor<float>(new[] { BatchSize, sequenceLength, hiddenSize });

                var tensorSize = BatchSize * sequenceLength * hiddenSize;
                var dataSlice = inputData!.AsSpan(0, Math.Min(tensorSize, inputData.Length));

                queryTensor.CopyFromHost(dataSlice);
                keyTensor.CopyFromHost(dataSlice);
                valueTensor.CopyFromHost(dataSlice);

                npuAccelerator.Execute(operation, 
                    new[] { queryTensor, keyTensor, valueTensor }, 
                    new[] { outputTensor });
                npuAccelerator.Synchronize();

                var result = outputTensor.CopyToHost();
                return result[0];
            }
            catch
            {
                return CPU_NeuralNetwork();
            }
        }

        [Benchmark]
        public float NPU_QuantizedInference()
        {
            if (npuAccelerator == null)
                return CPU_NeuralNetwork();

            try
            {
                var inputSize = Math.Min(NetworkSize, 512);
                var outputSize = inputSize / 2;

                // INT8 quantized inference - NPU specialty
                var operation = new NPUQuantizedInferenceOperation(
                    inputSize: inputSize,
                    outputSize: outputSize,
                    batchSize: BatchSize,
                    quantizationScale: 0.125f,
                    zeroPoint: 128
                );

                using var inputTensor = npuAccelerator.CreateTensor<sbyte>(new[] { BatchSize, inputSize });
                using var weightTensor = npuAccelerator.CreateTensor<sbyte>(new[] { outputSize, inputSize });
                using var biasTensor = npuAccelerator.CreateTensor<int>(new[] { outputSize });
                using var outputTensor = npuAccelerator.CreateTensor<sbyte>(new[] { BatchSize, outputSize });

                // Quantize input data
                var quantizedInput = new sbyte[BatchSize * inputSize];
                var quantizedWeights = new sbyte[outputSize * inputSize];
                var quantizedBias = new int[outputSize];

                for (int i = 0; i < Math.Min(quantizedInput.Length, inputData!.Length); i++)
                {
                    quantizedInput[i] = (sbyte)Math.Clamp(inputData[i] / 0.125f + 128, -128, 127);
                }
                for (int i = 0; i < Math.Min(quantizedWeights.Length, weightsData!.Length); i++)
                {
                    quantizedWeights[i] = (sbyte)Math.Clamp(weightsData[i] / 0.125f + 128, -128, 127);
                }
                for (int i = 0; i < Math.Min(quantizedBias.Length, biasData!.Length); i++)
                {
                    quantizedBias[i] = (int)(biasData[i] / (0.125f * 0.125f));
                }

                inputTensor.CopyFromHost(quantizedInput);
                weightTensor.CopyFromHost(quantizedWeights);
                biasTensor.CopyFromHost(quantizedBias);

                npuAccelerator.Execute(operation, 
                    new[] { inputTensor, weightTensor, biasTensor }, 
                    new[] { outputTensor });
                npuAccelerator.Synchronize();

                var result = outputTensor.CopyToHost();
                return result[0] * 0.125f - 128; // Dequantize result
            }
            catch
            {
                return CPU_NeuralNetwork();
            }
        }

        [Benchmark]
        public float NPU_EdgeAI_Pipeline()
        {
            if (npuAccelerator == null)
                return CPU_NeuralNetwork();

            try
            {
                // Simulate edge AI pipeline: preprocessing -> inference -> postprocessing
                var imageSize = 32;
                var numClasses = 10;

                var preprocessOp = new NPUPreprocessingOperation(imageSize, imageSize, 3);
                var inferenceOp = new NPUClassificationOperation(imageSize * imageSize * 3, numClasses);
                var postprocessOp = new NPUSoftmaxOperation(numClasses);

                using var rawInput = npuAccelerator.CreateTensor<float>(new[] { BatchSize, 3, imageSize, imageSize });
                using var preprocessed = npuAccelerator.CreateTensor<float>(new[] { BatchSize, 3, imageSize, imageSize });
                using var logits = npuAccelerator.CreateTensor<float>(new[] { BatchSize, numClasses });
                using var probabilities = npuAccelerator.CreateTensor<float>(new[] { BatchSize, numClasses });

                using var weights = npuAccelerator.CreateTensor<float>(new[] { numClasses, 3 * imageSize * imageSize });

                var inputSize = BatchSize * 3 * imageSize * imageSize;
                var weightSize = numClasses * 3 * imageSize * imageSize;

                rawInput.CopyFromHost(inputData!.AsSpan(0, Math.Min(inputSize, inputData.Length)));
                weights.CopyFromHost(weightsData!.AsSpan(0, Math.Min(weightSize, weightsData.Length)));

                // Execute pipeline
                npuAccelerator.Execute(preprocessOp, new[] { rawInput }, new[] { preprocessed });
                npuAccelerator.Execute(inferenceOp, new[] { preprocessed, weights }, new[] { logits });
                npuAccelerator.Execute(postprocessOp, new[] { logits }, new[] { probabilities });
                npuAccelerator.Synchronize();

                var result = probabilities.CopyToHost();
                return result[0];
            }
            catch
            {
                return CPU_NeuralNetwork();
            }
        }
#else
        [Benchmark]
        public float NPU_NotAvailable()
        {
            // NPU not available on this platform
            return CPU_NeuralNetwork();
        }
#endif

        public void Dispose()
        {
            Cleanup();
        }
    }
}

